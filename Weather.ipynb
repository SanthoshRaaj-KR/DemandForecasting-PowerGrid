{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e6d6b6-ae04-4c81-8420-97b492e46e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_climatic_data(lat, lng, date):\n",
    "    \"\"\"\n",
    "    Fetches daily climatic data for a specific coordinate and date.\n",
    "    'date' should be a string in 'YYYY-MM-DD' format or a datetime object.\n",
    "    \"\"\"\n",
    "    # Ensure date is in string format for the API\n",
    "    date_str = pd.to_datetime(date).strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lng,\n",
    "        \"start_date\": date_str,\n",
    "        \"end_date\": date_str,\n",
    "        \"daily\": (\n",
    "            \"temperature_2m_mean,\"\n",
    "            \"temperature_2m_max,\"\n",
    "            \"dewpoint_2m_mean,\"\n",
    "            \"relative_humidity_2m_mean,\"\n",
    "            \"wind_speed_10m_mean,\"\n",
    "            \"wind_gusts_10m_max,\"\n",
    "            \"precipitation_sum,\"\n",
    "            \"shortwave_radiation_sum\"\n",
    "        ),\n",
    "        \"timezone\": \"Asia/Kolkata\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status() # Check for request errors\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the 'daily' dictionary (contains lists of 1 element since we requested 1 day)\n",
    "        daily_data = data.get(\"daily\", {})\n",
    "        \n",
    "        # Clean up: Return a simple dictionary instead of lists of length 1\n",
    "        result = {key: value[0] for key, value in daily_data.items()}\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {lat}, {lng} on {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# lat_val = 16.50  # Andhra Pradesh\n",
    "# lng_val = 80.64\n",
    "# target_date = \"2023-05-15\"\n",
    "\n",
    "# weather_info = get_climatic_data(lat_val, lng_val, target_date)\n",
    "# print(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c3564d-33a4-4b0f-9261-71f5b618cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '2020-01-01', 'temperature_2m_mean': 24.5, 'temperature_2m_max': 28.4, 'dewpoint_2m_mean': 20.2, 'relative_humidity_2m_mean': 77, 'wind_speed_10m_mean': 13.2, 'wind_gusts_10m_max': 40.3, 'precipitation_sum': 1.4, 'shortwave_radiation_sum': 12.28}\n"
     ]
    }
   ],
   "source": [
    "print(get_climatic_data(16.50, 80.64,'2020-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b312b96f-6c2c-46d7-ae5c-dc73e1bc4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_nasa_power_weather(lat, lon, date):\n",
    "    # NASA uses YYYYMMDD format\n",
    "    date_str = pd.to_datetime(date).strftime('%Y%m%d')\n",
    "    \n",
    "    # Parameters: T2M (Temp), RH2M (Humidity), ALLSKY_SFC_SW_DWN (Solar Radiation)\n",
    "    url = (f\"https://power.larc.nasa.gov/api/temporal/daily/point?\"\n",
    "           f\"parameters=T2M_MAX,T2M_MIN,RH2M,ALLSKY_SFC_SW_DWN,PRECTOTCORR&\"\n",
    "           f\"community=RE&longitude={lon}&latitude={lat}&\"\n",
    "           f\"start={date_str}&end={date_str}&format=JSON\")\n",
    "    \n",
    "    response = requests.get(url).json()\n",
    "    # Extract data from the complex NASA JSON structure\n",
    "    data = response['properties']['parameter']\n",
    "    \n",
    "    return {\n",
    "        \"temp_max\": data['T2M_MAX'][date_str],\n",
    "        \"humidity\": data['RH2M'][date_str],\n",
    "        \"solar_radiation\": data['ALLSKY_SFC_SW_DWN'][date_str],\n",
    "        \"precipitation\": data['PRECTOTCORR'][date_str]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd93ae40-4e10-443a-a0ae-7e3a44086148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temp_max': 30.6, 'humidity': 78.1, 'solar_radiation': 4.0243, 'precipitation': 0.15}\n"
     ]
    }
   ],
   "source": [
    "print(get_nasa_power_weather(16.50, 80.64,'2020-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1f1201-4716-4c49-a66e-955462832a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading India_Elec_data_(Jan2020-Mar2025).csv...\n",
      "Fetching extended weather variables for 36 states...\n",
      "  Fetching: Andhra Pradesh\n",
      "  Skipping: Arunachal Pradesh (No coords)\n",
      "  Fetching: Assam\n",
      "  Fetching: Bihar\n",
      "  Fetching: Chandigarh\n",
      "  Skipping: Chhattisgarh (No coords)\n",
      "  Skipping: DD (No coords)\n",
      "  Skipping: DNH (No coords)\n",
      "  Fetching: DVC\n",
      "  Fetching: Delhi\n",
      "  Skipping: ER Odisha (No coords)\n",
      "  Skipping: Essar steel (No coords)\n",
      "  Skipping: Goa (No coords)\n",
      "  Fetching: Gujarat\n",
      "  Fetching: HP\n",
      "  Fetching: Haryana\n",
      "  Fetching: J&K(UT) & Ladakh(UT)\n",
      "  Fetching: Jharkhand\n",
      "  Fetching: Kerala\n",
      "  Fetching: MP\n",
      "  Fetching: Manipur\n",
      "Open-Meteo Error: 'daily'\n",
      "  Fetching: Mizoram\n",
      "Open-Meteo Error: 'daily'\n",
      "  Fetching: NER Meghalaya\n",
      "Open-Meteo Error: 'daily'\n",
      "  Fetching: NR UP\n",
      "Open-Meteo Error: 'daily'\n",
      "  Skipping: Nagaland (No coords)\n",
      "  Fetching: Puducherry\n",
      "Open-Meteo Error: 'daily'\n",
      "  Fetching: Punjab\n",
      "Open-Meteo Error: 'daily'\n",
      "  Fetching: Rajasthan\n",
      "  Skipping: SR Karnataka (No coords)\n",
      "  Skipping: Sikkim (No coords)\n",
      "  Fetching: Tamil Nadu\n",
      "  Fetching: Telangana\n",
      "  Fetching: Tripura\n",
      "  Skipping: Uttarakhand (No coords)\n",
      "  Fetching: WR Maharashtra\n",
      "  Fetching: West Bengal\n",
      "\n",
      "Success! Master file saved with all Open-Meteo and NASA columns.\n",
      "Columns added: om_temp_mean, om_wind_gusts, nasa_solar, etc.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD & CALCULATE 'ACTUAL DRAWL'\n",
    "# ==========================================\n",
    "file_path = 'India_Elec_data_(Jan2020-Mar2025).csv'\n",
    "print(f\"Loading {file_path}...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Rename tricky columns\n",
    "df.rename(columns={\n",
    "    'Drawl Schedule': 'Drawl_Schedule',\n",
    "    'OD(+) / UD(-)': 'OD_UD',\n",
    "    'Max Demand Met': 'Max_Demand_Met',\n",
    "    'Shortage During Peak': 'Peak_Shortage'\n",
    "}, inplace=True)\n",
    "\n",
    "# Calculate Actual_Drawl (Schedule + OD/UD)\n",
    "df['Drawl_Schedule'] = pd.to_numeric(df['Drawl_Schedule'], errors='coerce').fillna(0)\n",
    "df['OD_UD'] = pd.to_numeric(df['OD_UD'], errors='coerce').fillna(0)\n",
    "df['Actual_Drawl'] = df['Drawl_Schedule'] + df['OD_UD']\n",
    "\n",
    "# Ensure Date is datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: CITY MAPPING & COORDINATES\n",
    "# ==========================================\n",
    "city_map = {\n",
    "    'HP': 'Shimla', 'Telangana': 'Hyderabad', 'Punjab': 'Ludhiana', 'NR UP': 'Lucknow',\n",
    "    'MP': 'Bhopal', 'West Bengal': 'Kolkata', 'DD': 'Daman', 'SR Karnataka': 'Bengaluru',\n",
    "    'Delhi': 'New Delhi', 'Kerala': 'Thiruvananthapuram', 'Haryana': 'Chandigarh',\n",
    "    'Gujarat': 'Ahmedabad', 'Chhattisgarh': 'Raipur', 'ER Odisha': 'Bhubaneswar',\n",
    "    'Andhra Pradesh': 'Vijayawada', 'Tamil Nadu': 'Chennai', 'Bihar': 'Patna',\n",
    "    'WR Maharashtra': 'Mumbai', 'Assam': 'Guwahati', 'Rajasthan': 'Jaipur',\n",
    "    'Essar steel': 'Hazira', 'Uttarakhand': 'Dehradun', 'DVC': 'Kolkata',\n",
    "    'NER Meghalaya': 'Shillong', 'J&K(UT) & Ladakh(UT)': 'Srinagar', 'Chandigarh': 'Chandigarh',\n",
    "    'Puducherry': 'Puducherry', 'Tripura': 'Agartala', 'DNH': 'Silvassa', 'Jharkhand': 'Ranchi',\n",
    "    'Goa': 'Panaji', 'Mizoram': 'Aizawl', 'Nagaland': 'Kohima', 'Manipur': 'Imphal',\n",
    "    'Arunachal Pradesh': 'Itanagar', 'Sikkim': 'Gangtok'\n",
    "}\n",
    "\n",
    "def get_lat_long(state_name):\n",
    "    search_query = city_map.get(state_name, state_name)\n",
    "    params = {\n",
    "       'api_key': 'AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv',\n",
    "       's': search_query, 'stack': 'aws', 'locale': 'en', 'filter': 'international',\n",
    "       'place-types': 'settlement,airport,district', 'order': 'importance', 'a': 'true', 'format': 'json'\n",
    "    }\n",
    "    try:\n",
    "        url = 'https://locator-service.api.bbci.co.uk/locations?' + urlencode(params)\n",
    "        result = requests.get(url).json()\n",
    "        components = result['response']['results']['results'][0]\n",
    "        return components['latitude'], components['longitude']\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: BULK WEATHER FETCHING (UPDATED)\n",
    "# ==========================================\n",
    "def fetch_bulk_weather_custom(lat, lng, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches FULL range using the specific parameters you requested.\n",
    "    \"\"\"\n",
    "    s_str = start_date.strftime('%Y-%m-%d')\n",
    "    e_str = end_date.strftime('%Y-%m-%d')\n",
    "    s_nasa = start_date.strftime('%Y%m%d')\n",
    "    e_nasa = end_date.strftime('%Y%m%d')\n",
    "\n",
    "    # --- 1. Open-Meteo (All requested columns) ---\n",
    "    try:\n",
    "        om_url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "        # Using exact parameters from your snippet\n",
    "        om_params = {\n",
    "            \"latitude\": lat, \"longitude\": lng, \n",
    "            \"start_date\": s_str, \"end_date\": e_str, \n",
    "            \"daily\": \"temperature_2m_mean,temperature_2m_max,dewpoint_2m_mean,relative_humidity_2m_mean,wind_speed_10m_mean,wind_gusts_10m_max,precipitation_sum,shortwave_radiation_sum\",\n",
    "            \"timezone\": \"Asia/Kolkata\"\n",
    "        }\n",
    "        om_res = requests.get(om_url, params=om_params).json()\n",
    "        \n",
    "        # Create DataFrame with prefix 'om_' to avoid collisions\n",
    "        df_om = pd.DataFrame({\n",
    "            'Date': pd.to_datetime(om_res['daily']['time']),\n",
    "            'om_temp_mean': om_res['daily']['temperature_2m_mean'],\n",
    "            'om_temp_max': om_res['daily']['temperature_2m_max'],\n",
    "            'om_dewpoint': om_res['daily']['dewpoint_2m_mean'],\n",
    "            'om_humidity': om_res['daily']['relative_humidity_2m_mean'],\n",
    "            'om_wind_speed': om_res['daily']['wind_speed_10m_mean'],\n",
    "            'om_wind_gusts': om_res['daily']['wind_gusts_10m_max'],\n",
    "            'om_precip': om_res['daily']['precipitation_sum'],\n",
    "            'om_solar': om_res['daily']['shortwave_radiation_sum']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Open-Meteo Error: {e}\")\n",
    "        df_om = pd.DataFrame()\n",
    "\n",
    "    # --- 2. NASA POWER (All requested columns) ---\n",
    "    try:\n",
    "        # Using exact parameters from your snippet: T2M_MAX, RH2M, ALLSKY_SFC_SW_DWN, PRECTOTCORR\n",
    "        nasa_url = f\"https://power.larc.nasa.gov/api/temporal/daily/point?parameters=T2M_MAX,RH2M,ALLSKY_SFC_SW_DWN,PRECTOTCORR&community=RE&longitude={lng}&latitude={lat}&start={s_nasa}&end={e_nasa}&format=JSON\"\n",
    "        nasa_res = requests.get(nasa_url).json()['properties']['parameter']\n",
    "        \n",
    "        df_nasa = pd.DataFrame({\n",
    "            'Date': pd.to_datetime(list(nasa_res['T2M_MAX'].keys())),\n",
    "            'nasa_temp_max': list(nasa_res['T2M_MAX'].values()),\n",
    "            'nasa_humidity': list(nasa_res['RH2M'].values()),\n",
    "            'nasa_solar': list(nasa_res['ALLSKY_SFC_SW_DWN'].values()),\n",
    "            'nasa_precip': list(nasa_res['PRECTOTCORR'].values())\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"NASA Error: {e}\")\n",
    "        df_nasa = pd.DataFrame()\n",
    "\n",
    "    # Merge logic\n",
    "    if df_om.empty and df_nasa.empty: return pd.DataFrame()\n",
    "    elif df_om.empty: return df_nasa\n",
    "    elif df_nasa.empty: return df_om\n",
    "    else: return df_om.merge(df_nasa, on='Date', how='outer')\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: EXECUTION\n",
    "# ==========================================\n",
    "all_weather_data = []\n",
    "unique_states = df['State'].unique()\n",
    "start_dt, end_dt = df['Date'].min(), df['Date'].max()\n",
    "\n",
    "print(f\"Fetching extended weather variables for {len(unique_states)} states...\")\n",
    "\n",
    "for state in unique_states:\n",
    "    lat, lng = get_lat_long(state)\n",
    "    if lat and lng:\n",
    "        print(f\"  Fetching: {state}\")\n",
    "        # Call the new custom function\n",
    "        sw = fetch_bulk_weather_custom(lat, lng, start_dt, end_dt)\n",
    "        if not sw.empty:\n",
    "            sw['State'] = state\n",
    "            all_weather_data.append(sw)\n",
    "        time.sleep(1) # Be nice to the API\n",
    "    else:\n",
    "        print(f\"  Skipping: {state} (No coords)\")\n",
    "\n",
    "# Final Merge\n",
    "if all_weather_data:\n",
    "    full_weather_df = pd.concat(all_weather_data)\n",
    "    final_df = df.merge(full_weather_df, on=['Date', 'State'], how='left', suffixes=('', '_drop'))\n",
    "    final_df = final_df[[c for c in final_df.columns if not c.endswith('_drop')]]\n",
    "    \n",
    "    # Calculate Gap\n",
    "    final_df['Gap'] = final_df['Actual_Drawl'] - final_df['Drawl_Schedule']\n",
    "    \n",
    "    # Save\n",
    "    final_df.to_csv('Master_Weather_Electricity_Data.csv', index=False)\n",
    "    print(\"\\nSuccess! Master file saved with all Open-Meteo and NASA columns.\")\n",
    "    print(\"Columns added: om_temp_mean, om_wind_gusts, nasa_solar, etc.\")\n",
    "else:\n",
    "    print(\"Failed to fetch weather data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b042a5-84d0-43b8-af82-1d5c75fcb549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Master_Weather_Electricity_Data.csv to patch missing states...\n",
      "Patching 17 states...\n",
      "Fixing: Arunachal Pradesh ... Done.\n",
      "Fixing: Chhattisgarh ... Done.\n",
      "Fixing: DD ... Done.\n",
      "Fixing: DNH ... Done.\n",
      "Fixing: ER Odisha ... Done.\n",
      "Fixing: Essar steel ...   > NASA failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Done.\n",
      "Fixing: Goa ... Done.\n",
      "Fixing: Nagaland ... Done.\n",
      "Fixing: SR Karnataka ... Done.\n",
      "Fixing: Sikkim ... Done.\n",
      "Fixing: Uttarakhand ... Done.\n",
      "Fixing: Manipur ... Done.\n",
      "Fixing: Mizoram ...   > Open-Meteo failed: 'daily'\n",
      "Done.\n",
      "Fixing: NER Meghalaya ...   > Open-Meteo failed: 'daily'\n",
      "Done.\n",
      "Fixing: NR UP ... Done.\n",
      "Fixing: Puducherry ... Done.\n",
      "Fixing: Punjab ... Done.\n",
      "\n",
      "------------------------------------------------\n",
      "COMPLETE! Fixed file saved as: 'Master_Weather_Electricity_Data_FIXED.csv'\n",
      "Total Rows: 69012\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD EXISTING MASTER FILE\n",
    "# ==========================================\n",
    "file_path = 'Master_Weather_Electricity_Data.csv'\n",
    "print(f\"Loading {file_path} to patch missing states...\")\n",
    "df = pd.read_csv(file_path)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEFINE MANUAL COORDINATES FOR FAILED STATES\n",
    "# ==========================================\n",
    "# These are the specific states that failed in your run\n",
    "FAILED_STATES = {\n",
    "    # --- No Coords Errors ---\n",
    "    'Arunachal Pradesh': (27.08, 93.60),\n",
    "    'Chhattisgarh': (21.27, 81.63),\n",
    "    'DD': (20.41, 72.83),  # Daman\n",
    "    'DNH': (20.26, 73.00), # Silvassa\n",
    "    'ER Odisha': (20.29, 85.82), # Bhubaneswar\n",
    "    'Essar steel': (21.08, 72.63), # Hazira\n",
    "    'Goa': (15.49, 73.82),\n",
    "    'Nagaland': (25.67, 94.10),\n",
    "    'SR Karnataka': (12.97, 77.59), # Bengaluru\n",
    "    'Sikkim': (27.33, 88.61),\n",
    "    'Uttarakhand': (30.31, 78.03),\n",
    "    \n",
    "    # --- Open-Meteo Errors (Likely Rate Limit/Coords) ---\n",
    "    'Manipur': (24.81, 93.93),\n",
    "    'Mizoram': (23.73, 92.71),\n",
    "    'NER Meghalaya': (25.57, 91.88),\n",
    "    'NR UP': (26.84, 80.95), # Lucknow\n",
    "    'Puducherry': (11.94, 79.80),\n",
    "    'Punjab': (30.73, 76.77)\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 3. WEATHER FETCHING FUNCTION\n",
    "# ==========================================\n",
    "def fetch_weather_patch(lat, lng, start_date, end_date):\n",
    "    s_str = start_date.strftime('%Y-%m-%d')\n",
    "    e_str = end_date.strftime('%Y-%m-%d')\n",
    "    s_nasa = start_date.strftime('%Y%m%d')\n",
    "    e_nasa = end_date.strftime('%Y%m%d')\n",
    "\n",
    "    # API 1: Open-Meteo\n",
    "    try:\n",
    "        om_url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "        om_params = {\n",
    "            \"latitude\": lat, \"longitude\": lng, \"start_date\": s_str, \"end_date\": e_str, \n",
    "            \"daily\": \"temperature_2m_mean,temperature_2m_max,dewpoint_2m_mean,relative_humidity_2m_mean,wind_speed_10m_mean,wind_gusts_10m_max,precipitation_sum,shortwave_radiation_sum\",\n",
    "            \"timezone\": \"Asia/Kolkata\"\n",
    "        }\n",
    "        # Increased timeout and added delay\n",
    "        om_res = requests.get(om_url, params=om_params, timeout=20).json()\n",
    "        df_om = pd.DataFrame({\n",
    "            'Date': pd.to_datetime(om_res['daily']['time']),\n",
    "            'om_temp_mean': om_res['daily']['temperature_2m_mean'],\n",
    "            'om_temp_max': om_res['daily']['temperature_2m_max'],\n",
    "            'om_dewpoint': om_res['daily']['dewpoint_2m_mean'],\n",
    "            'om_humidity': om_res['daily']['relative_humidity_2m_mean'],\n",
    "            'om_wind_speed': om_res['daily']['wind_speed_10m_mean'],\n",
    "            'om_wind_gusts': om_res['daily']['wind_gusts_10m_max'],\n",
    "            'om_precip': om_res['daily']['precipitation_sum'],\n",
    "            'om_solar': om_res['daily']['shortwave_radiation_sum']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  > Open-Meteo failed: {e}\")\n",
    "        df_om = pd.DataFrame()\n",
    "\n",
    "    # API 2: NASA POWER\n",
    "    try:\n",
    "        nasa_url = f\"https://power.larc.nasa.gov/api/temporal/daily/point?parameters=T2M_MAX,RH2M,ALLSKY_SFC_SW_DWN,PRECTOTCORR&community=RE&longitude={lng}&latitude={lat}&start={s_nasa}&end={e_nasa}&format=JSON\"\n",
    "        nasa_res = requests.get(nasa_url, timeout=20).json()['properties']['parameter']\n",
    "        df_nasa = pd.DataFrame({\n",
    "            'Date': pd.to_datetime(list(nasa_res['T2M_MAX'].keys())),\n",
    "            'nasa_temp_max': list(nasa_res['T2M_MAX'].values()),\n",
    "            'nasa_humidity': list(nasa_res['RH2M'].values()),\n",
    "            'nasa_solar': list(nasa_res['ALLSKY_SFC_SW_DWN'].values()),\n",
    "            'nasa_precip': list(nasa_res['PRECTOTCORR'].values())\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  > NASA failed: {e}\")\n",
    "        df_nasa = pd.DataFrame()\n",
    "\n",
    "    if df_om.empty and df_nasa.empty: return pd.DataFrame()\n",
    "    elif df_om.empty: return df_nasa\n",
    "    elif df_nasa.empty: return df_om\n",
    "    else: return df_om.merge(df_nasa, on='Date', how='outer')\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION LOOP\n",
    "# ==========================================\n",
    "start_dt, end_dt = df['Date'].min(), df['Date'].max()\n",
    "weather_cols = [c for c in df.columns if 'om_' in c or 'nasa_' in c]\n",
    "cleaned_rows = []\n",
    "\n",
    "print(f\"Patching {len(FAILED_STATES)} states...\")\n",
    "\n",
    "for state, coords in FAILED_STATES.items():\n",
    "    lat, lng = coords\n",
    "    print(f\"Fixing: {state} ... \", end=\"\")\n",
    "    \n",
    "    # Fetch new weather\n",
    "    new_weather = fetch_weather_patch(lat, lng, start_dt, end_dt)\n",
    "    \n",
    "    if not new_weather.empty:\n",
    "        # Get the rows for this state from original DF\n",
    "        state_df = df[df['State'] == state].copy()\n",
    "        \n",
    "        # Drop old broken weather columns if they exist\n",
    "        state_df = state_df.drop(columns=[c for c in weather_cols if c in state_df.columns], errors='ignore')\n",
    "        \n",
    "        # Merge new weather\n",
    "        merged_state = state_df.merge(new_weather, on='Date', how='left')\n",
    "        cleaned_rows.append(merged_state)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        # If it still fails, keep original rows\n",
    "        cleaned_rows.append(df[df['State'] == state])\n",
    "        print(\"Failed again (Keeping original).\")\n",
    "    \n",
    "    time.sleep(2) # Generous delay to prevent errors\n",
    "\n",
    "# ==========================================\n",
    "# 5. RECOMBINE AND SAVE\n",
    "# ==========================================\n",
    "# Keep states that were ALREADY good\n",
    "good_states = df[~df['State'].isin(FAILED_STATES.keys())]\n",
    "\n",
    "# Combine good states + fixed states\n",
    "if cleaned_rows:\n",
    "    fixed_df = pd.concat(cleaned_rows)\n",
    "    final_df = pd.concat([good_states, fixed_df], axis=0)\n",
    "    \n",
    "    # Sort nicely\n",
    "    final_df = final_df.sort_values(['State', 'Date'])\n",
    "    \n",
    "    final_df.to_csv('Master_Weather_Electricity_Data_FIXED.csv', index=False)\n",
    "    print(\"\\n------------------------------------------------\")\n",
    "    print(\"COMPLETE! Fixed file saved as: 'Master_Weather_Electricity_Data_FIXED.csv'\")\n",
    "    print(f\"Total Rows: {len(final_df)}\")\n",
    "else:\n",
    "    print(\"No updates were made.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
