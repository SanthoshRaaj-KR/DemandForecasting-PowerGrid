import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


data = pd.read_csv('India_Elec_data_(Jan2020-Mar2025).csv')


data.describe()


data["Date"] = pd.to_datetime(data["Date"], errors="coerce")


STATE = "Andhra Pradesh"
df = data[data["State"] == STATE].copy()


df = df.dropna(subset=["Drawl Schedule", "OD(+) / UD(-)"])


df["Actual_Drawl"] = df["Drawl Schedule"] + df["OD(+) / UD(-)"]


df = df.sort_values("Date").reset_index(drop=True)





corr = df["Drawl Schedule"].corr(df["Actual_Drawl"])
print("Correlation (S_t, A_t):", corr)





df["Error"] = df["Actual_Drawl"] - df["Drawl Schedule"]

error_stats = {
    "mean_error": df["Error"].mean(),
    "std_error": df["Error"].std(),
    "max_overdraw": df["Error"].max(),
    "max_underdraw": df["Error"].min()
}

print(error_stats)





df["Rel_OD"] = df["Error"] / df["Drawl Schedule"]

rel_stats = {
    "mean_rel_od": df["Rel_OD"].mean(),
    "std_rel_od": df["Rel_OD"].std(),
    "max_rel_od": df["Rel_OD"].max(),
    "min_rel_od": df["Rel_OD"].min()
}

print(rel_stats)





WINDOW = 100  # days

df["OD_roll_mean"] = df["Error"].rolling(WINDOW).mean()
df["OD_roll_std"]  = df["Error"].rolling(WINDOW).std()


for lag in [1, 2, 3, 7]:
    df[f"OD_lag_{lag}"] = df["Error"].shift(lag)
    corr_lag = df[f"OD_lag_{lag}"].corr(df["Actual_Drawl"])
    print(f"Corr(OD_t-{lag}, A_t): {corr_lag}")


from sklearn.linear_model import LinearRegression

X = df[["Drawl Schedule"]]
y = df["Actual_Drawl"]

model = LinearRegression()
model.fit(X, y)

print("Slope:", model.coef_[0])
print("Intercept:", model.intercept_)
print("R^2:", model.score(X, y))





df = pd.read_csv("India_Elec_data_(Jan2020-Mar2025).csv")


df["Date"] = pd.to_datetime(df["Date"], errors="coerce")

# Drop rows needed for drawl logic
df = df.dropna(subset=["Drawl Schedule", "OD(+) / UD(-)"])

# Actual Drawl
df["Actual_Drawl"] = df["Drawl Schedule"] + df["OD(+) / UD(-)"]

# Error (OD)
df["Error"] = df["Actual_Drawl"] - df["Drawl Schedule"]

# Relative OD 
df["Rel_OD"] = np.where(
    df["Drawl Schedule"] != 0,
    df["Error"] / df["Drawl Schedule"],
    np.nan
)



state_summary = (
    df.groupby("State")
      .agg(
          mean_scheduled_drawl=("Drawl Schedule", "mean"),
          mean_actual_drawl=("Actual_Drawl", "mean"),
          mean_error=("Error", "mean"),
          std_error=("Error", "std"),
          mean_rel_od=("Rel_OD", "mean"),
          std_rel_od=("Rel_OD", "std"),
          observations=("Error", "count")
      )
      .reset_index()
)


corr_list = []

for state, sdf in df.groupby("State"):
    if len(sdf) > 30:
        corr = sdf["Drawl Schedule"].corr(sdf["Actual_Drawl"])
        corr_list.append({"State": state, "corr_S_A": corr})

corr_df = pd.DataFrame(corr_list)

# Merge correlation into summary
state_summary = state_summary.merge(corr_df, on="State", how="left")

# Sort by predictability
state_summary = state_summary.sort_values(
    by="corr_S_A", ascending=False
).reset_index(drop=True)

print(state_summary)


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the Master Database
# Use the file you just generated (either _FIXED or the normal one)
file_path = 'Master_Weather_Electricity_Data_FIXED.csv' 
print(f"Loading {file_path} for Feature Selection...")
df = pd.read_csv(file_path)


# 2. Prepare Data
# Ensure we have the target 'Gap' (Actual - Schedule)
# We fill NaNs with 0 to ensure correlations don't break, though dropping is often better
df['Drawl_Schedule'] = pd.to_numeric(df['Drawl_Schedule'], errors='coerce').fillna(0)
df['Actual_Drawl'] = pd.to_numeric(df['Actual_Drawl'], errors='coerce').fillna(0)
df['Gap'] = df['Actual_Drawl'] - df['Drawl_Schedule']

# Identify all Weather Columns (Open-Meteo & NASA)
weather_cols = [c for c in df.columns if c.startswith('om_') or c.startswith('nasa_')]
print(f"Analyzing {len(weather_cols)} weather variables: {weather_cols}")


# 3. CALCULATE CORRELATIONS
results = {}

for state in df['State'].unique():
    state_data = df[df['State'] == state]
    
    # We need enough data points to run a correlation
    if len(state_data) > 50:
        # Calculate correlation of all weather cols against 'Gap'
        # We use .corrwith() to do it in one shot against the target series
        corrs = state_data[weather_cols].corrwith(state_data['Gap'])
        results[state] = corrs


# 4. CONVERT TO DATAFRAME
corr_df = pd.DataFrame(results).T # Transpose so States are rows, Weather Vars are columns

# 5. CALCULATE AGGREGATE SCORES (The "Significance")
# We take the ABSOLUTE mean because a strong negative correlation (-0.8) is just as important as positive
avg_scores = corr_df.abs().mean()

# Add the 'AVERAGE_SCORE' row to the bottom of the dataframe
corr_df.loc['AVERAGE_IMPORTANCE'] = avg_scores

# Sort columns by their Global Importance (High to Low)
sorted_cols = avg_scores.sort_values(ascending=False).index
corr_df = corr_df[sorted_cols]


# 6. PRINT RESULTS
print("\n" + "="*80)
print("DETAILED WEATHER IMPACT SCORES (Correlation with Demand Gap)")
print("="*80)
# Print the full table (States x Variables)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
print(corr_df.round(3))

print("\n" + "="*80)
print("FINAL VERDICT: VARIABLE SIGNIFICANCE RANKING")
print("="*80)
print(avg_scores.sort_values(ascending=False).round(4))

# 7. RECOMMENDATION: WHICH TO DROP?
# Define a threshold (e.g., 0.05). If average impact is lower, it's noise.
threshold = 0.05
drop_cols = avg_scores[avg_scores < threshold].index.tolist()
keep_cols = avg_scores[avg_scores >= threshold].index.tolist()

print("\n" + "="*80)
print("RECOMMENDATION")
print("="*80)
print(f"KEEP these columns (Significant Impact): {keep_cols}")
print(f"DROP these columns (Minimal/No Impact): {drop_cols}")


import pandas as pd

# 1. Load the fixed master file
file_path = 'Master_Weather_Electricity_Data_FIXED.csv' 
# (Or 'Master_Weather_Electricity_Data.csv' if you didn't need the patch)
df = pd.read_csv(file_path)

# 2. Select ONLY the Champions (The variables with the highest scores)
# We also keep the core columns: Date, State, Drawl info
columns_to_keep = [
    'Date', 
    'State', 
    'Actual_Drawl', 
    'Drawl_Schedule', 
    'Gap',            # The target we are analyzing
    'om_temp_mean',   # The Best Temp Metric
    'nasa_solar',     # The Best Solar Metric
    'om_precip',      # The Best Rain Metric
    'om_dewpoint',    # The Best Humidity Metric (Dewpoint combines heat + humidity)
    'om_wind_gusts'   # The Best Wind Metric
]

# 3. Create the Final Model-Ready Dataset
final_df = df[columns_to_keep].copy()

# 4. Save it
final_df.to_csv('Final_Model_Data.csv', index=False)

print("---------------------------------------------------")
print("DATA CLEANING COMPLETE")
print("---------------------------------------------------")
print(f"Original Columns: {len(df.columns)}")
print(f"Final Columns:    {len(final_df.columns)}")
print("Saved as: 'Final_Model_Data.csv'")
print("You are now ready to run the prediction model.")


import pandas as pd

# 1. Load the Data
file_path = 'Final_Model_Data.csv' 
# (Or 'Master_Weather_Electricity_Data.csv' if you haven't filtered features yet)
df = pd.read_csv(file_path)

print(f"Original Shape: {df.shape} (Rows, Cols)")

# ==================================================
# PART A: THE NULL REPORT
# ==================================================
print("\n--- NULL REPORT (Missing Values %) ---")
null_counts = df.isnull().sum()
null_percent = (df.isnull().sum() / len(df)) * 100

# Create a readable table
null_df = pd.DataFrame({'Missing_Rows': null_counts, 'Percent_Missing': null_percent})
print(null_df[null_df['Missing_Rows'] > 0].sort_values('Percent_Missing', ascending=False))

# ==================================================
# PART B: THE CLEAN UP STRATEGY
# ==================================================
# Rule 1: Drop COLUMNS that are more than 30% empty (Too much missing info)
threshold_col = 30 # Percentage
cols_to_drop = null_percent[null_percent > threshold_col].index.tolist()

if cols_to_drop:
    print(f"\n[ACTION] Dropping {len(cols_to_drop)} columns (> {threshold_col}% missing):")
    print(cols_to_drop)
    df = df.drop(columns=cols_to_drop)
else:
    print(f"\n[ACTION] No columns exceeded the {threshold_col}% missing threshold.")

# Rule 2: Drop ROWS for whatever small missing data is left
# (e.g. if one day is missing temp, we can't train on it)
rows_before = len(df)
df = df.dropna()
rows_after = len(df)
lost_rows = rows_before - rows_after

print(f"\n[ACTION] Dropped {lost_rows} rows containing NaNs.")

# ==================================================
# PART C: FINAL SAVE
# ==================================================
print(f"\nFinal Clean Shape: {df.shape}")
df.to_csv('Final_Cleaned_Dataset.csv', index=False)
print("Saved as: 'Final_Cleaned_Dataset.csv'")


import pandas as pd

# 1. Load the Three Datasets
try:
    df_orig = pd.read_csv('India_Elec_data_(Jan2020-Mar2025).csv')
    df_model = pd.read_csv('Final_Model_Data.csv')       # Before cleaning nulls
    df_clean = pd.read_csv('Final_Cleaned_Dataset.csv')  # After cleaning nulls

    # 2. Get Row Counts
    rows_orig = len(df_orig)
    rows_model = len(df_model)
    rows_clean = len(df_clean)

    # 3. Print the Reduction Report
    print("--- ROW REDUCTION REPORT ---")
    print(f"1. Original Data:       {rows_orig:,} rows")
    print(f"2. After Weather Merge: {rows_model:,} rows (Difference: {rows_model - rows_orig:,})")
    print(f"3. After Cleaning Nulls:{rows_clean:,} rows (Dropped: {rows_model - rows_clean:,} rows)")

    # 4. Calculate Percentage Loss
    total_loss = rows_orig - rows_clean
    loss_pct = (total_loss / rows_orig) * 100
    print("-" * 30)
    print(f"Total Rows Lost: {total_loss:,}")
    print(f"Data Retention:  {100 - loss_pct:.2f}%")
    print("-" * 30)

except FileNotFoundError as e:
    print(f"Error: Could not find one of the files. {e}")


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the Final Cleaned Data
file_path = 'Final_Cleaned_Dataset.csv'
print(f"Loading {file_path}...")
df = pd.read_csv(file_path)

# 2. Define the Columns to Analyze
# These are the "Champions" we selected earlier
champion_cols = [
    'om_temp_mean',   # Avg Temperature (The main driver)
    'nasa_solar',     # Solar Radiation
    'om_dewpoint',    # Humidity/Heat Index
    'om_wind_gusts',  # Wind
    'om_precip'       # Rain
]
target = 'Actual_Drawl'  # Total Consumption

# 3. Safety Check: Ensure all columns exist and are numeric
print(f"Analyzing {len(df)} rows of data...")
available_cols = [c for c in champion_cols if c in df.columns]

if not available_cols:
    print("ERROR: Champion columns not found! Check your CSV headers.")
else:
    # Convert to numeric just in case (coercing errors to NaN)
    cols_to_fix = available_cols + [target]
    for col in cols_to_fix:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # 4. Calculate Correlation
    # We compare every Champion against 'Actual_Drawl'
    correlation_matrix = df[available_cols + [target]].corr()
    
    # Extract just the target column, sorted from highest to lowest
    target_corr = correlation_matrix[[target]].drop(target).sort_values(by=target, ascending=False)

    # 5. Print the Text Report
    print("\n" + "="*50)
    print("CORRELATION REPORT: Weather vs. Actual Demand")
    print("="*50)
    print(target_corr)
    print("\n" + "-"*50)
    print("INTERPRETATION:")
    print("High Positive (> 0.6) : Strong Driver (Hotter = More Power)")
    print("Negative (< -0.1)     : Inverse Driver (Rain = Less Power)")
    print("Near Zero (-0.1 to 0.1): No Impact")
    print("-" * 50)

    # 6. Generate Heatmap Visualization
    plt.figure(figsize=(10, 6))
    sns.heatmap(target_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=1)
    plt.title('Impact of Weather Champions on Total Electricity Demand')
    plt.show()


import pandas as pd

# Load your data
df = pd.read_csv('Final_Cleaned_Dataset.csv')

# Calculate Correlation PER STATE
# This removes the "Size Difference" problem
state_corrs = df.groupby('State')[['Actual_Drawl', 'om_temp_mean']].corr().iloc[0::2, -1]
print(state_corrs.sort_values(ascending=False))



